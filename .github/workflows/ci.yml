name: CI/Performance Monitoring

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
    
    - name: Install UV
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.cargo/bin" >> $GITHUB_PATH
    
    - name: Install dependencies
      run: |
        uv sync
    
    - name: Run Tests with Performance Reporting
      env:
        # Core test environment configuration
        OPENAI_API_KEY: test-key-for-mocks
        VECTOR_DATABASE: qdrant
        DATABASE_PROVIDER: qdrant
        QDRANT_URL: http://localhost:6333
        QDRANT_API_KEY: ""
        QDRANT_COLLECTION_NAME: test_crawled_pages
        QDRANT_EMBEDDING_MODEL: text-embedding-3-small
        SEARXNG_URL: http://localhost:8081
        SEARXNG_TEST_URL: http://localhost:8081
        USE_RERANKING: "false"
        RERANKER_MODEL: BAAI/bge-reranker-v2-m3
        CHUNK_SIZE: "2000"
        CHUNK_OVERLAP: "200"
        TEST_TIMEOUT: "30"
        TEST_PARALLEL_WORKERS: "4"
        PYTEST_VERBOSE: "true"
        PYTHONPATH: ${{ github.workspace }}/src
        TESTING: "true"
        CI: "true"
      run: |
        # Run tests with both performance monitoring and json report
        uv run pytest --json-report --json-report-file=test_report.json --perf-monitor --perf-output=performance_metrics.json || true
        
        # Check if test report was generated
        if [ -f "test_report.json" ]; then
          echo "Test report generated successfully"
        else
          echo "Warning: test_report.json not found"
        fi
        
        # Check if performance metrics were generated
        if [ -f "performance_metrics.json" ]; then
          echo "Performance metrics generated successfully"
        else
          echo "Warning: performance_metrics.json not found"
        fi
    
    - name: Generate Performance Dashboard
      if: always()
      run: |
        # Always try to generate the dashboard, even if tests failed
        uv run python scripts/generate_performance_dashboard.py performance_metrics.json || echo "Failed to generate performance dashboard"
    
    - name: Upload Performance Dashboard
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: performance-dashboard
        path: |
          performance_dashboard.html
          test_report.json
          performance_metrics.json
        if-no-files-found: warn
        retention-days: 14