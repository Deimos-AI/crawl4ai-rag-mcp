name: Qdrant QA Automation

permissions:
  contents: read
  pull-requests: write
  issues: write
  checks: write

on:
  workflow_dispatch:
    inputs:
      run_benchmarks:
        description: 'Run performance benchmarks'
        required: false
        default: 'true'
        type: boolean
      run_stress_tests:
        description: 'Run stress tests'
        required: false
        default: 'true'
        type: boolean
  pull_request:
    paths:
      - 'src/database/qdrant_adapter.py'
      - 'src/database/factory.py'
      - 'src/utils_refactored.py'
      - 'tests/test_qdrant_*.py'
      - 'tests/benchmark_qdrant.py'
  push:
    branches:
      - main
      - develop
    paths:
      - 'src/database/qdrant_adapter.py'

jobs:
  qdrant-qa:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    strategy:
      matrix:
        python-version: ['3.12']
    
    services:
      qdrant:
        image: qdrant/qdrant:latest
        ports:
          - 6333:6333
        options: >-
          --health-cmd "curl -f http://127.0.0.1:6333/readyz || exit 1"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 10
          --health-start-period 30s
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install UV
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.cargo/bin" >> $GITHUB_PATH
    
    - name: Cache dependencies
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/uv
          .venv
        key: ${{ runner.os }}-uv-${{ hashFiles('pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-uv-
    
    - name: Install dependencies
      run: |
        uv sync
    
    - name: Verify test dependencies
      run: |
        echo "🔍 Verifying test dependencies..."
        uv run pip list | grep -E "(pytest|pytest-cov|pytest-mock)" || echo "⚠️ Some test dependencies might be missing"
        uv run pytest --version
        echo ""
        echo "📦 All installed packages:"
        uv run pip list
    
    - name: Debug environment
      run: |
        echo "🔍 Environment information:"
        echo "Python version: $(python --version)"
        echo "Current directory: $(pwd)"
        echo "Docker containers:"
        docker ps -a || true
        echo "Network services:"
        netstat -tuln | grep -E "(6333|8051)" || true
    
    - name: Verify Qdrant is healthy
      run: |
        max_attempts=30
        attempt=0
        echo "🔍 Checking Qdrant health status..."
        
        while [ $attempt -lt $max_attempts ]; do
          # Try to get the health status
          response=$(curl -s -w "\n%{http_code}" http://localhost:6333/readyz 2>/dev/null || echo "CURL_FAILED")
          http_code=$(echo "$response" | tail -n1)
          body=$(echo "$response" | head -n-1)
          
          echo "Attempt $((attempt + 1))/$max_attempts - HTTP Code: $http_code"
          
          # Check if we got a successful response (2xx status code)
          if [[ "$http_code" =~ ^2[0-9][0-9]$ ]]; then
            echo "✅ Qdrant is healthy! Response: $body"
            break
          fi
          
          # Also accept if the response contains common success indicators
          if echo "$body" | grep -qiE "(ok|ready|true|operational)" 2>/dev/null; then
            echo "✅ Qdrant is healthy! Response: $body"
            break
          fi
          
          echo "⏳ Waiting for Qdrant to become healthy..."
          sleep 2
          attempt=$((attempt + 1))
        done
        
        if [ $attempt -eq $max_attempts ]; then
          echo "❌ Qdrant failed to become healthy after $max_attempts attempts!"
          echo "Last response: $response"
          echo ""
          echo "📋 Docker container status:"
          docker ps -a | grep -E "(qdrant|6333)" || echo "No Qdrant container found"
          echo ""
          echo "📜 Qdrant container logs:"
          docker logs $(docker ps -aq --filter "ancestor=qdrant/qdrant") 2>&1 || echo "Could not retrieve logs"
          exit 1
        fi
    
    - name: Run unit tests
      env:
        VECTOR_DATABASE: qdrant
        QDRANT_URL: http://localhost:6333
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY || 'test-key-for-mocks' }}
      run: |
        echo "🧪 Running Qdrant adapter unit tests..."
        echo "Environment variables:"
        echo "  VECTOR_DATABASE=$VECTOR_DATABASE"
        echo "  QDRANT_URL=$QDRANT_URL"
        echo ""
        
        # Run tests with error handling
        if uv run pytest tests/test_qdrant_adapter.py -vv --tb=short; then
          echo "✅ Unit tests passed!"
        else
          echo "❌ Unit tests failed!"
          echo ""
          echo "📋 Checking test file exists:"
          ls -la tests/test_qdrant_adapter.py || echo "Test file not found!"
          exit 1
        fi
    
    - name: Run integration tests
      env:
        VECTOR_DATABASE: qdrant
        QDRANT_URL: http://localhost:6333
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY || 'test-key-for-mocks' }}
      run: |
        echo "🔗 Running Qdrant integration tests..."
        
        # Verify Qdrant is still accessible
        curl -s http://localhost:6333/readyz || echo "⚠️ Warning: Qdrant might not be accessible"
        
        # Run tests with error handling
        if uv run pytest tests/test_qdrant_integration.py -vv --tb=short; then
          echo "✅ Integration tests passed!"
        else
          echo "❌ Integration tests failed!"
          echo ""
          echo "📋 Checking test file exists:"
          ls -la tests/test_qdrant_integration.py || echo "Test file not found!"
          exit 1
        fi
    
    - name: Run performance benchmarks
      if: ${{ github.event.inputs.run_benchmarks != 'false' }}
      env:
        VECTOR_DATABASE: qdrant
        QDRANT_URL: http://localhost:6333
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY || 'test-key-for-mocks' }}
      run: |
        echo "📊 Running performance benchmarks..."
        
        # Check if benchmark script exists
        if [ -f "tests/benchmark_qdrant.py" ]; then
          uv run python tests/benchmark_qdrant.py | tee benchmark_results.txt
          echo "✅ Benchmarks completed!"
        else
          echo "⚠️ Benchmark script not found, skipping..."
          echo "No benchmarks run" > benchmark_results.txt
        fi
    
    - name: Run interface contract tests
      env:
        VECTOR_DATABASE: qdrant
        QDRANT_URL: http://localhost:6333
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY || 'test-key-for-mocks' }}
      run: |
        echo "📋 Running interface contract tests..."
        
        # Check if test file and class exist
        if [ -f "tests/test_database_interface.py" ]; then
          # Try to run the specific test class
          if uv run pytest tests/test_database_interface.py::TestQdrantInterface -vv --tb=short; then
            echo "✅ Interface contract tests passed!"
          else
            echo "⚠️ TestQdrantInterface not found or failed, trying all interface tests..."
            uv run pytest tests/test_database_interface.py -vv --tb=short || echo "⚠️ Interface tests failed"
          fi
        else
          echo "⚠️ Interface test file not found, skipping..."
        fi
    
    - name: Generate test coverage
      env:
        VECTOR_DATABASE: qdrant
        QDRANT_URL: http://localhost:6333
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY || 'test-key-for-mocks' }}
      run: |
        echo "📊 Generating test coverage report..."
        uv run pytest tests/test_qdrant_*.py \
          --cov=src/database/qdrant_adapter \
          --cov=src/database/factory \
          --cov-report=term-missing \
          --cov-report=xml \
          --cov-report=html
    
    - name: Upload coverage reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: coverage-reports-${{ matrix.python-version }}
        path: |
          coverage.xml
          htmlcov/
          .coverage
        if-no-files-found: warn
    
    - name: Generate test report
      if: always()
      run: |
        echo "# Qdrant QA Test Report" > test_report.md
        echo "## Test Results Summary" >> test_report.md
        echo "" >> test_report.md
        
        if [ -f "benchmark_results.txt" ]; then
          echo "### Performance Benchmarks" >> test_report.md
          echo '```' >> test_report.md
          cat benchmark_results.txt >> test_report.md
          echo '```' >> test_report.md
        fi
        
        echo "" >> test_report.md
        echo "### Test Execution Details" >> test_report.md
        echo "- Python Version: ${{ matrix.python-version }}" >> test_report.md
        echo "- Qdrant URL: http://localhost:6333" >> test_report.md
        echo "- Execution Time: $(date)" >> test_report.md
        echo "" >> test_report.md
        
        # Add test status
        if [ "${{ job.status }}" == "success" ]; then
          echo "## ✅ Overall Result: PASSED" >> test_report.md
        else
          echo "## ❌ Overall Result: FAILED" >> test_report.md
        fi
    
    - name: Upload test report
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: qdrant-qa-report
        path: |
          test_report.md
          benchmark_results.txt
          qdrant_test_logs.txt
    
    - name: Comment PR with results
      if: github.event_name == 'pull_request' && always()
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          let comment = '## 🧪 Qdrant QA Test Results\n\n';
          
          if ('${{ job.status }}' === 'success') {
            comment += '✅ **All tests passed!**\n\n';
          } else {
            comment += '❌ **Some tests failed.**\n\n';
          }
          
          // Add benchmark results if available
          try {
            const benchmarkResults = fs.readFileSync('benchmark_results.txt', 'utf8');
            comment += '### Performance Benchmarks\n```\n' + benchmarkResults + '\n```\n';
          } catch (e) {
            // No benchmark results
          }
          
          comment += '\n[View full test report](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})';
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });
    
    - name: Check test results
      if: always()
      run: |
        if [ "${{ job.status }}" != "success" ]; then
          echo "❌ Tests failed!"
          exit 1
        fi
        echo "✅ All tests passed!"

  docker-integration:
    runs-on: ubuntu-latest
    needs: qdrant-qa
    if: github.event_name == 'push' || github.event_name == 'workflow_dispatch'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Create test environment file
      run: |
        cp .env.example .env.test
        echo "VECTOR_DATABASE=qdrant" >> .env.test
        echo "QDRANT_URL=http://qdrant:6333" >> .env.test
        echo "OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY || 'test-key' }}" >> .env.test
    
    - name: Create Qdrant compose file
      run: |
        cat > docker-compose.qdrant.yml << 'EOF'
        version: '3.8'
        
        services:
          qdrant:
            image: qdrant/qdrant:latest
            container_name: qdrant
            ports:
              - "6333:6333"
            volumes:
              - qdrant-data:/qdrant/storage
            networks:
              - default
            healthcheck:
              test: ["CMD", "/usr/bin/bash", "-c", "exec 3<>/dev/tcp/127.0.0.1/6333 && echo -e 'GET /readyz HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: close\\r\\n\\r\\n' >&3 && grep -q 'HTTP/1.1 200' <&3"]
              interval: 10s
              timeout: 5s
              retries: 5
              start_period: 30s
        
        volumes:
          qdrant-data:
            driver: local
        EOF
    
    - name: Start Docker services
      run: |
        docker compose -f docker-compose.yml -f docker-compose.qdrant.yml --env-file .env.test up -d
    
    - name: Wait for services
      run: |
        echo "Waiting for services to be healthy..."
        sleep 10
        
        # Wait for Qdrant
        timeout 60 bash -c 'until curl -s http://localhost:6333/readyz | grep -q "All systems operational"; do sleep 1; done'
        
        # Wait for MCP server
        timeout 60 bash -c 'until curl -s http://localhost:8051/health; do sleep 1; done'
    
    - name: Test MCP tools with Qdrant
      run: |
        # Test basic connectivity
        docker compose exec -T mcp-crawl4ai python -c "
        import asyncio
        from database.factory import create_database_client
        
        async def test():
            client = create_database_client()
            print('✅ Successfully connected to Qdrant via Docker!')
        
        asyncio.run(test())
        "
    
    - name: Collect Docker logs
      if: always()
      run: |
        docker compose logs > docker_logs.txt
    
    - name: Upload Docker logs
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: docker-logs
        path: docker_logs.txt
    
    - name: Cleanup
      if: always()
      run: |
        docker compose -f docker-compose.yml -f docker-compose.qdrant.yml down -v